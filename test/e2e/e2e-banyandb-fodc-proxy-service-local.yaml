# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# E2E test for FODC proxy service functionality

setup:
  env: kind
  file: kind.yaml
  init-system-environment: env
  kind:
    expose-ports:
      - namespace: istio-system
        resource: service/skywalking-ui
        port: 80
  steps:
    - name: set PATH
      command: export PATH=/tmp/skywalking-infra-e2e/bin:$PATH
    - name: install yq
      command: bash test/e2e/setup-e2e-shell/install.sh yq
    - name: install swctl
      command: bash test/e2e/setup-e2e-shell/install.sh swctl
    - name: install kubectl
      command: bash test/e2e/setup-e2e-shell/install.sh kubectl
    - name: install istio
      command: |
        export PATH=/tmp/skywalking-infra-e2e/bin:$PATH
        bash test/e2e/setup-e2e-shell/install.sh istioctl
        ISTIO_VERSION="1.28.2"
        CLUSTER_NAME="kind"
        echo "Pre-pulling and loading Istio images (version: $ISTIO_VERSION) into kind cluster..."
        # Check Docker Hub authentication (helps avoid rate limits)
        if ! docker info &>/dev/null; then
          echo "Warning: Docker is not accessible"
        elif ! docker login &>/dev/null 2>&1; then
          echo "Warning: Not authenticated with Docker Hub. Run 'docker login' to avoid rate limits."
        fi
        # Pre-pull and load all Istio images needed for demo profile into kind cluster nodes
        # This bypasses Docker Hub rate limits during pod startup
        # Demo profile needs: pilot (istiod), proxyv2 (for gateways and sidecars)
        ISTIO_IMAGES="istio/pilot:${ISTIO_VERSION} istio/proxyv2:${ISTIO_VERSION}"
        for image in ${ISTIO_IMAGES}; do
          FULL_IMAGE="docker.io/${image}"
          echo "Processing ${FULL_IMAGE}..."
          # Check if image already exists locally
          if docker image inspect ${FULL_IMAGE} &>/dev/null; then
            echo "Image ${FULL_IMAGE} already exists locally"
          else
            # Try to pull with retries
            PULLED=false
            for i in {1..3}; do
              echo "Pulling ${FULL_IMAGE} (attempt ${i}/3)..."
              if docker pull ${FULL_IMAGE} 2>&1; then
                echo "Successfully pulled ${FULL_IMAGE}"
                PULLED=true
                break
              else
                echo "Failed to pull ${FULL_IMAGE}, attempt ${i}/3"
                if [ $i -lt 3 ]; then
                  sleep 15
                fi
              fi
            done
            if [ "$PULLED" = false ]; then
              echo "Warning: Failed to pull ${FULL_IMAGE} after 3 attempts."
              echo "Istio installation will attempt to pull it, but may hit Docker Hub rate limits."
              continue
            fi
          fi
          # Load image into kind cluster nodes (bypasses Docker Hub during pod startup)
          echo "Loading ${FULL_IMAGE} into kind cluster nodes..."
          if kind load docker-image ${FULL_IMAGE} --name ${CLUSTER_NAME} 2>&1; then
            echo "Successfully loaded ${FULL_IMAGE} into kind cluster"
          else
            echo "Note: Image may already be present in kind cluster nodes"
          fi
        done
        echo "Istio images prepared. Starting Istio installation..."
        echo "Installing Istio..."
        # Use minimal profile - no gateways, just Istiod and sidecar injection
        # This avoids gateway timeout issues and is sufficient for FODC proxy test
        istioctl install -y --set profile=minimal \
          --set meshConfig.defaultConfig.envoyAccessLogService.address=skywalking-oap.istio-system:11800 \
          --set meshConfig.enableEnvoyAccessLogService=true
        kubectl label namespace default istio-injection=enabled
      wait:
        - namespace: istio-system
          resource: pod
          for: condition=ready
          label-selector: app=istiod
    - name: Install helm
      command: bash test/e2e/setup-e2e-shell/install.sh helm
    - name: Install dependency
      command: |
        helm repo add bitnami https://charts.bitnami.com/bitnami
        cd chart
        # Retry logic for network issues with exponential backoff
        max_retries=5
        retry_count=0
        delay=10
        while [ $retry_count -lt $max_retries ]; do
          echo "Attempting helm dependency update (attempt $((retry_count + 1))/$max_retries)..."
          if helm dependency update; then
            echo "Successfully updated chart dependencies"
            break
          else
            retry_count=$((retry_count + 1))
            if [ $retry_count -lt $max_retries ]; then
              echo "Helm dependency update failed (attempt $retry_count/$max_retries)"
              echo "This might be due to Docker Hub rate limiting or network issues."
              echo "Waiting ${delay} seconds before retry..."
              sleep $delay
              delay=$((delay + 10))
            else
              echo "ERROR: Failed to update chart dependencies after $max_retries attempts"
              echo "Common causes: Docker Hub rate limiting, network issues, or firewall/proxy blocking"
              exit 1
            fi
          fi
        done
    - name: Pre-load required images
      command: |
        export PATH=/tmp/skywalking-infra-e2e/bin:$PATH
        CLUSTER_NAME="kind"
        PULL_TIMEOUT=300  # 5 minutes per image
        LOAD_TIMEOUT=60    # 1 minute per load
        echo "Pre-loading required images to avoid Docker Hub rate limits..."
        # Helper function for timeout (works on both Linux and macOS)
        run_with_timeout() {
          local timeout=$1
          shift
          if command -v timeout >/dev/null 2>&1; then
            timeout ${timeout} "$@"
          elif command -v gtimeout >/dev/null 2>&1; then
            gtimeout ${timeout} "$@"
          else
            # Fallback: run in background with sleep-based timeout
            "$@" &
            local pid=$!
            local count=0
            while kill -0 $pid 2>/dev/null && [ $count -lt $timeout ]; do
              sleep 1
              count=$((count + 1))
            done
            if kill -0 $pid 2>/dev/null; then
              kill $pid 2>/dev/null
              wait $pid 2>/dev/null
              return 124  # Timeout exit code
            else
              wait $pid
              return $?
            fi
          fi
        }
        # Only pre-load essential images - reduce list to avoid hanging
        # Etcd uses bitnamilegacy/etcd image - Chart version 12.0.18 uses 3.6.4-debian-12-r3
        ETCD_IMAGES="bitnamilegacy/etcd:3.6.4-debian-12-r3"
        # BanyanDB images - using specific tag from ghcr.io
        BANYANDB_IMAGES="ghcr.io/apache/skywalking-banyandb:8a880f51df7a9f843803b84ca8b6e9ff9d2acfca ghcr.io/apache/skywalking-banyandb:8a880f51df7a9f843803b84ca8b6e9ff9d2acfca-slim"
        # FODC images - using specific tags from test (ghcr.io repository)
        FODC_IMAGES="ghcr.io/apache/skywalking-banyandb-fodc-proxy:ab3de19288c77701a38e46267ee98509e9d386c2 ghcr.io/apache/skywalking-banyandb-fodc-agent:ab3de19288c77701a38e46267ee98509e9d386c2"
        # Init container images - liaison pod uses busybox for wait-for-fodc-proxy init container
        INIT_IMAGES="docker.io/library/busybox:1.36"
        ALL_IMAGES="${ETCD_IMAGES} ${BANYANDB_IMAGES} ${FODC_IMAGES} ${INIT_IMAGES}"
        LOADED_ANY=false
        for image in ${ALL_IMAGES}; do
          # Some images already have full path (ghcr.io), some need docker.io prefix
          if [[ $image == docker.io/* ]] || [[ $image == ghcr.io/* ]]; then
            FULL_IMAGE="${image}"
          else
            FULL_IMAGE="docker.io/${image}"
          fi
          echo "Processing ${FULL_IMAGE}..."
          if docker image inspect ${FULL_IMAGE} &>/dev/null; then
            echo "Image ${FULL_IMAGE} already exists locally"
            LOADED_ANY=true
          else
            echo "Pulling ${FULL_IMAGE} (timeout: ${PULL_TIMEOUT}s)..."
            # Use timeout to prevent hanging
            if run_with_timeout ${PULL_TIMEOUT} docker pull ${FULL_IMAGE} 2>&1; then
              echo "Successfully pulled ${FULL_IMAGE}"
              LOADED_ANY=true
            else
              PULL_EXIT=$?
              if [ $PULL_EXIT -eq 124 ]; then
                echo "Warning: Pull timeout for ${FULL_IMAGE} (will try during Helm install)"
              else
                echo "Warning: Failed to pull ${FULL_IMAGE} (exit code: $PULL_EXIT, will try during Helm install)"
              fi
              continue
            fi
          fi
          echo "Loading ${FULL_IMAGE} into kind cluster nodes (timeout: ${LOAD_TIMEOUT}s)..."
          # Use timeout to prevent hanging
          if run_with_timeout ${LOAD_TIMEOUT} kind load docker-image ${FULL_IMAGE} --name ${CLUSTER_NAME} 2>&1; then
            echo "Successfully loaded ${FULL_IMAGE}"
          else
            LOAD_EXIT=$?
            if [ $LOAD_EXIT -eq 124 ]; then
              echo "Warning: Load timeout for ${FULL_IMAGE} (may already be in cluster)"
            else
              echo "Note: Failed to load ${FULL_IMAGE} (exit code: $LOAD_EXIT, may already be in cluster)"
            fi
          fi
        done
        if [ "$LOADED_ANY" = true ]; then
          echo "Required images prepared"
        else
          echo "Warning: No images were pre-loaded. Helm install will pull them (may hit rate limits)"
        fi
    - name: Install BanyanDB with FODC proxy
      command: |
        cd chart
        helm install -n istio-system banyandb-test . \
          --set cluster.enabled=true \
          --set cluster.fodcProxy.enabled=true \
          --set cluster.fodcProxy.grpcSvc.port=17904 \
          --set cluster.fodcProxy.httpSvc.port=17905 \
          --set cluster.fodcProxy.httpSvc.type=LoadBalancer \
          --set cluster.fodcAgent.enabled=true \
          --set cluster.fodcAgent.httpPort=17914 \
          --set cluster.fodcAgent.startupProbe.initialDelaySeconds=5 \
          --set cluster.fodcAgent.startupProbe.periodSeconds=5 \
          --set cluster.fodcAgent.startupProbe.failureThreshold=120 \
          --set cluster.fodcAgent.readinessProbe.initialDelaySeconds=5 \
          --set cluster.fodcAgent.readinessProbe.periodSeconds=10 \
          --set cluster.fodcAgent.readinessProbe.failureThreshold=60 \
          --set cluster.data.enabled=true \
          --set cluster.data.replicas=1 \
          --set cluster.liaison.enabled=true \
          --set cluster.liaison.replicas=1 \
          --set etcd.replicaCount=1 \
          --set image.repository=ghcr.io/apache/skywalking-banyandb \
          --set image.tag=8a880f51df7a9f843803b84ca8b6e9ff9d2acfca \
          --set cluster.fodcProxy.image.tag=ab3de19288c77701a38e46267ee98509e9d386c2 \
          --set cluster.fodcAgent.image.tag=ab3de19288c77701a38e46267ee98509e9d386c2
    - name: Load missing images if needed
      command: |
        export PATH=/tmp/skywalking-infra-e2e/bin:$PATH
        CLUSTER_NAME="kind"
        echo "Checking what images are being used by pods..."
        # Wait a moment for pods to be created
        sleep 5
        # Check etcd image
        ETCD_IMAGE=$(kubectl get pod -n istio-system -l app.kubernetes.io/name=etcd -o jsonpath='{.items[0].spec.containers[0].image}' 2>/dev/null || echo "")
        if [ -n "$ETCD_IMAGE" ] && [ "$ETCD_IMAGE" != "null" ]; then
          echo "Detected etcd pod using image: $ETCD_IMAGE"
          if docker image inspect ${ETCD_IMAGE} &>/dev/null; then
            echo "Loading ${ETCD_IMAGE} into kind cluster nodes..."
            kind load docker-image ${ETCD_IMAGE} --name ${CLUSTER_NAME} 2>&1 || echo "Note: Image may already be in cluster"
          fi
        fi
        # Check banyandb image
        BANYANDB_IMAGE=$(kubectl get pod -n istio-system -l app.kubernetes.io/name=banyandb -o jsonpath='{.items[0].spec.containers[0].image}' 2>/dev/null || echo "")
        if [ -n "$BANYANDB_IMAGE" ] && [ "$BANYANDB_IMAGE" != "null" ]; then
          echo "Detected banyandb pod using image: $BANYANDB_IMAGE"
          if ! docker image inspect ${BANYANDB_IMAGE} &>/dev/null; then
            echo "Image ${BANYANDB_IMAGE} not found locally, pulling..."
            if docker pull ${BANYANDB_IMAGE} 2>&1; then
              echo "Successfully pulled ${BANYANDB_IMAGE}"
            fi
          fi
          if docker image inspect ${BANYANDB_IMAGE} &>/dev/null; then
            echo "Loading ${BANYANDB_IMAGE} into kind cluster nodes..."
            kind load docker-image ${BANYANDB_IMAGE} --name ${CLUSTER_NAME} 2>&1 || echo "Note: Image may already be in cluster"
          fi
        else
          echo "BanyanDB pods not created yet or image not detected"
        fi
        # Check for init container images (e.g., busybox used by liaison pod)
        echo "Checking for init container images..."
        INIT_IMAGES=$(kubectl get pods -n istio-system -o jsonpath='{range .items[*]}{range .spec.initContainers[*]}{.image}{"\n"}{end}{end}' 2>/dev/null | sort -u || echo "")
        if [ -n "$INIT_IMAGES" ]; then
          for INIT_IMAGE in ${INIT_IMAGES}; do
            echo "Found init container image: ${INIT_IMAGE}"
            if ! docker image inspect ${INIT_IMAGE} &>/dev/null; then
              echo "Pulling ${INIT_IMAGE}..."
              if docker pull ${INIT_IMAGE} 2>&1; then
                echo "Successfully pulled ${INIT_IMAGE}"
              fi
            fi
            if docker image inspect ${INIT_IMAGE} &>/dev/null; then
              echo "Loading ${INIT_IMAGE} into kind cluster nodes..."
              kind load docker-image ${INIT_IMAGE} --name ${CLUSTER_NAME} 2>&1 || echo "Note: Image may already be in cluster"
            fi
          done
        fi
        # Debug: Show pod status
        echo "Current pod status:"
        kubectl get pods -n istio-system -l 'app.kubernetes.io/name in (etcd,banyandb)' 2>&1 || echo "Could not get pod status"
        # Show all banyandb pods with their labels
        echo "Banyandb pods with labels:"
        kubectl get pods -n istio-system --show-labels | grep banyandb || echo "No banyandb pods found"
        # Show events for debugging
        echo "Recent events:"
        kubectl get events -n istio-system --sort-by='.lastTimestamp' --field-selector involvedObject.kind=Pod | tail -30 2>&1 || echo "Could not get events"
        # Check why pods are pending - show full details
        echo "Checking pod details (full):"
        for pod in $(kubectl get pods -n istio-system -l 'app.kubernetes.io/name in (etcd,banyandb)' -o name 2>/dev/null); do
          echo "=== $pod ==="
          kubectl get $pod -n istio-system -o jsonpath='{.status.phase}{"\n"}' 2>&1 || echo "Could not get pod phase"
          kubectl describe $pod -n istio-system | tail -20 2>&1 || echo "Could not describe pod"
        done
        # Check PVC status (pods might be waiting for PVCs)
        echo "Checking PVC status:"
        kubectl get pvc -n istio-system 2>&1 || echo "Could not get PVC status"
        # Check storage class
        echo "Checking storage class:"
        kubectl get storageclass 2>&1 || echo "Could not get storage class"
        # Check specific pod: data-hot-1
        echo "=== Detailed check for data-hot-1 pod ==="
        echo "StatefulSet status:"
        kubectl get statefulset banyandb-test-data-hot -n istio-system -o jsonpath='{.status}' 2>&1 | head -20 || echo "Could not get StatefulSet status"
        echo "StatefulSet spec (replicas, podManagementPolicy):"
        kubectl get statefulset banyandb-test-data-hot -n istio-system -o jsonpath='{.spec.replicas} replicas, podManagementPolicy={.spec.podManagementPolicy}' 2>&1 && echo ""
        echo "Checking both data pods scheduling status:"
        for pod_name in banyandb-test-data-hot-0 banyandb-test-data-hot-1; do
          if kubectl get pod $pod_name -n istio-system &>/dev/null; then
            echo "--- $pod_name ---"
            echo "Pod YAML (spec.volumes to check PVC references):"
            kubectl get pod $pod_name -n istio-system -o jsonpath='{range .spec.volumes[*]}{.name}: {.persistentVolumeClaim.claimName}{"\n"}{end}' 2>&1 || echo "Could not get volumes"
            echo "Pod conditions (full):"
            kubectl get pod $pod_name -n istio-system -o jsonpath='{range .status.conditions[*]}{.type}={.status} reason={.reason} message={.message}{"\n"}{end}' 2>&1 || echo "No conditions"
            echo "Pod scheduled status:"
            kubectl get pod $pod_name -n istio-system -o jsonpath='{.status.conditions[?(@.type=="PodScheduled")]}' 2>&1 && echo "" || echo "PodScheduled condition not found"
            echo "Node name:"
            kubectl get pod $pod_name -n istio-system -o jsonpath='{.spec.nodeName}' 2>&1 && echo "" || echo "No node assigned"
            echo "Pod creation timestamp vs now:"
            kubectl get pod $pod_name -n istio-system -o jsonpath='Created: {.metadata.creationTimestamp}' 2>&1 && echo ""
            echo "All events for $pod_name (including from all namespaces):"
            kubectl get events --all-namespaces --field-selector involvedObject.name=$pod_name --sort-by='.lastTimestamp' 2>&1 | tail -20 || echo "No events found"
            echo "Checking if pod is in scheduler queue (describe should show unschedulable reason):"
            kubectl describe pod $pod_name -n istio-system | grep -A 10 "Events:" || echo "Could not describe pod"
          fi
        done
        echo "Checking if PVCs are blocking scheduling:"
        echo "PVCs for data-hot-0:"
        kubectl get pvc -n istio-system | grep "data-hot-0" || echo "No PVCs found"
        echo "PVCs for data-hot-1:"
        kubectl get pvc -n istio-system | grep "data-hot-1" || echo "No PVCs found"
        echo "Checking node resources:"
        kubectl top nodes 2>&1 || echo "Could not get node metrics (metrics-server may not be available)"
        kubectl describe nodes | grep -A 5 "Allocated resources" || echo "Could not get node resource details"
        # Check FODC proxy service accessibility
        echo "Checking FODC proxy service:"
        kubectl get svc -n istio-system banyandb-test-fodc-proxy-http 2>&1 || echo "FODC proxy HTTP service not found"
        echo "Testing FODC proxy connectivity from a test pod:"
        kubectl run test-proxy-connect --image=busybox:1.36 --rm -i --restart=Never -n istio-system -- sh -c "wget -q -T 5 -O- http://banyandb-test-fodc-proxy-http:17905/metrics 2>&1 | head -5" || echo "Could not test proxy connectivity"
        # Check FODC agent container status and logs
        echo "Checking FODC agent container status:"
        for pod in $(kubectl get pods -n istio-system -l app.kubernetes.io/name=banyandb -o name 2>/dev/null); do
          echo "=== $pod full status ==="
          kubectl get $pod -n istio-system -o jsonpath='{.status.phase}' 2>&1 && echo ""
          echo "Init containers:"
          kubectl get $pod -n istio-system -o jsonpath='{range .status.initContainerStatuses[*]}{.name}: ready={.ready} state={.state}{"\n"}{end}' 2>&1 || echo "No init containers or not started"
          echo "Containers:"
          kubectl get $pod -n istio-system -o jsonpath='{range .status.containerStatuses[*]}{.name}: ready={.ready} started={.started} restartCount={.restartCount} state={.state}{"\n"}{end}' 2>&1 || echo "No containers or not started"
          echo "=== $pod fodc-agent logs (last 30 lines) ==="
          kubectl logs $pod -n istio-system -c fodc-agent --tail=30 2>&1 || echo "Could not get fodc-agent logs (container may not exist or not started)"
          echo "=== $pod init container logs (if any) ==="
          for init_container in $(kubectl get $pod -n istio-system -o jsonpath='{range .spec.initContainers[*]}{.name}{"\n"}{end}' 2>/dev/null); do
            echo "--- Init container $init_container logs ---"
            kubectl logs $pod -n istio-system -c $init_container --tail=30 2>&1 || echo "Could not get logs for $init_container"
          done
        done
        echo "Waiting for pods to become ready (this may take several minutes for PVCs to bind and containers to start)..."
      wait:
        - namespace: istio-system
          resource: pod
          for: condition=ready
          label-selector: app.kubernetes.io/name=banyandb
        - namespace: istio-system
          resource: deployments/banyandb-test-fodc-proxy
          for: condition=available
    - name: Install SkyWalking
      command: |
        helm -n istio-system install --timeout 10m skywalking oci://ghcr.io/apache/skywalking-helm/skywalking-helm \
                       --version "$SW_HELM_VERSION" \
                       --set fullnameOverride=skywalking \
                       --set oap.env.SW_ENVOY_METRIC_ALS_HTTP_ANALYSIS=k8s-mesh \
                       --set oap.env.SW_ENVOY_METRIC_ALS_TCP_ANALYSIS=k8s-mesh \
                       --set oap.env.K8S_SERVICE_NAME_RULE='e2e::${service.metadata.name}' \
                       --set oap.env.SW_STORAGE_BANYANDB_USER=admin \
                       --set oap.env.SW_STORAGE_BANYANDB_PASSWORD=banyandb \
                       --set oap.replicas=1 \
                       --set ui.image.repository=$UI_REPO \
                       --set ui.image.tag=$UI_TAG \
                       --set oap.image.repository=$OAP_REPO \
                       --set oap.image.tag=$OAP_TAG \
                       --set oap.storageType=banyandb \
                       --set elasticsearch.enabled=false \
                       --set banyandb.enabled=false \
                       -f test/e2e/values.yaml
      wait:
        - namespace: istio-system
          resource: deployments/skywalking-oap
          for: condition=available
    - name: Deploy demo services
      command: |
        kubectl apply -f https://raw.githubusercontent.com/istio/istio/$ISTIO_VERSION/samples/bookinfo/platform/kube/bookinfo.yaml
        kubectl apply -f https://raw.githubusercontent.com/istio/istio/$ISTIO_VERSION/samples/bookinfo/networking/bookinfo-gateway.yaml
        # Enable TCP services
        kubectl apply -f https://raw.githubusercontent.com/istio/istio/$ISTIO_VERSION/samples/bookinfo/platform/kube/bookinfo-ratings-v2.yaml
        kubectl apply -f https://raw.githubusercontent.com/istio/istio/$ISTIO_VERSION/samples/bookinfo/platform/kube/bookinfo-db.yaml
        kubectl apply -f https://raw.githubusercontent.com/istio/istio/$ISTIO_VERSION/samples/bookinfo/networking/destination-rule-all.yaml
        kubectl apply -f https://raw.githubusercontent.com/istio/istio/$ISTIO_VERSION/samples/bookinfo/networking/virtual-service-ratings-db.yaml
      wait:
        - namespace: default
          resource: pod
          for: condition=Ready
    - name: Generate traffic
      path: traffic-gen.yaml
      wait:
        - namespace: default
          resource: pod
          for: condition=Ready
  timeout: 25m

verify:
  retry:
    count: 20
    interval: 15s
  cases:
    # Verify FODC proxy is collecting observability data - check services are observable
    - query: swctl --display yaml --base-url=http://${service_skywalking_ui_host}:${service_skywalking_ui_80}/graphql service ls
      expected: expected/service.yml
    # Verify FODC agent is working - check metrics are available for services
    - query: swctl --display yaml --base-url=http://${service_skywalking_ui_host}:${service_skywalking_ui_80}/graphql metrics exec --expression=service_sla --service-name=e2e::productpage
      expected: expected/metrics-has-value.yml
    - query: swctl --display yaml --base-url=http://${service_skywalking_ui_host}:${service_skywalking_ui_80}/graphql metrics exec --expression=service_cpm --service-name=e2e::productpage
      expected: expected/metrics-has-value.yml
